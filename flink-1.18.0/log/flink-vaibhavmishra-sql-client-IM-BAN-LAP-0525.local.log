2024-12-13 11:44:32,622 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m
2024-12-13 11:44:32,623 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: env.java.opts.jobmanager, -Duser.timezone=UTC
2024-12-13 11:44:32,623 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.bind-host, localhost
2024-12-13 11:44:32,623 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: env.java.opts, -Duser.timezone=UTC
2024-12-13 11:44:32,623 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
2024-12-13 11:44:32,623 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: table.local-time-zone, UTC
2024-12-13 11:44:32,623 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, localhost
2024-12-13 11:44:32,623 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.memory.process.size, 1600m
2024-12-13 11:44:32,623 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123
2024-12-13 11:44:32,623 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.bind-address, localhost
2024-12-13 11:44:32,623 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.bind-host, localhost
2024-12-13 11:44:32,623 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.host, localhost
2024-12-13 11:44:32,623 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 1
2024-12-13 11:44:32,623 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
2024-12-13 11:44:32,623 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.address, localhost
2024-12-13 11:44:32,623 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: env.java.opts.taskmanager, -Duser.timezone=UTC
2024-12-13 11:44:32,623 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: env.java.opts.all, --add-exports=java.base/sun.net.util=ALL-UNNAMED --add-exports=java.rmi/sun.rmi.registry=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED --add-exports=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.text=ALL-UNNAMED --add-opens=java.base/java.time=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.locks=ALL-UNNAMED
2024-12-13 11:44:32,635 INFO  org.apache.flink.client.cli.CliFrontend                      [] - Loading FallbackYarnSessionCli
2024-12-13 11:44:32,646 INFO  org.apache.flink.core.fs.FileSystem                          [] - Hadoop is not in the classpath/dependencies. The extended set of supported File Systems via Hadoop is not available.
2024-12-13 11:44:32,655 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-statsd
2024-12-13 11:44:32,658 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-prometheus
2024-12-13 11:44:32,658 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-graphite
2024-12-13 11:44:32,658 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: external-resource-gpu
2024-12-13 11:44:32,658 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-jmx
2024-12-13 11:44:32,658 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-influx
2024-12-13 11:44:32,658 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-slf4j
2024-12-13 11:44:32,658 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID not found, creating it: metrics-datadog
2024-12-13 11:44:32,672 INFO  org.apache.flink.table.gateway.service.context.DefaultContext [] - Execution config: {execution.savepoint.ignore-unclaimed-state=false, execution.attached=true, execution.savepoint-restore-mode=NO_CLAIM, execution.shutdown-on-attached-exit=false, pipeline.jars=[file:/Users/vaibhavmishra/projects/self/flink-poc/flink-1.18.0/opt/flink-sql-client-1.18.0.jar, file:/Users/vaibhavmishra/projects/self/flink-poc/flink-1.18.0/opt/flink-python-1.18.0.jar], pipeline.classpaths=[], execution.target=remote}
2024-12-13 11:44:32,953 INFO  org.apache.flink.configuration.Configuration                 [] - Config uses fallback configuration key 'rest.port' instead of key 'rest.bind-port'
2024-12-13 11:44:32,973 INFO  org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint   [] - Starting rest endpoint.
2024-12-13 11:44:33,113 INFO  org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint   [] - Rest endpoint listening at localhost:52952
2024-12-13 11:44:33,114 INFO  org.apache.flink.table.client.SqlClient                      [] - Start embedded gateway on port 52952
2024-12-13 11:44:33,210 INFO  org.apache.flink.table.client.gateway.ExecutorImpl           [] - Open session to http://localhost:52952 with connection version: V2.
2024-12-13 11:44:33,377 INFO  org.apache.flink.table.client.cli.CliClient                  [] - Command history file path: /Users/vaibhavmishra/.flink-sql-history
2024-12-13 11:45:33,183 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.sql.parser.ddl.SqlCreateCatalog does not contain a setter for field catalogName
2024-12-13 11:45:33,184 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - Class class org.apache.flink.sql.parser.ddl.SqlCreateCatalog cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
2024-12-13 11:45:33,184 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.sql.parser.ddl.SqlCreateView does not contain a setter for field viewName
2024-12-13 11:45:33,184 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - Class class org.apache.flink.sql.parser.ddl.SqlCreateView cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
2024-12-13 11:45:33,185 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.sql.parser.ddl.SqlAlterViewRename does not contain a getter for field newViewIdentifier
2024-12-13 11:45:33,185 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.sql.parser.ddl.SqlAlterViewRename does not contain a setter for field newViewIdentifier
2024-12-13 11:45:33,185 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - Class class org.apache.flink.sql.parser.ddl.SqlAlterViewRename cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
2024-12-13 11:45:33,186 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.sql.parser.ddl.SqlAlterViewProperties does not contain a setter for field propertyList
2024-12-13 11:45:33,186 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - Class class org.apache.flink.sql.parser.ddl.SqlAlterViewProperties cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
2024-12-13 11:45:33,186 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.sql.parser.ddl.SqlAlterViewAs does not contain a setter for field newQuery
2024-12-13 11:45:33,186 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - Class class org.apache.flink.sql.parser.ddl.SqlAlterViewAs cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
2024-12-13 11:45:33,186 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.sql.parser.ddl.SqlAddPartitions does not contain a setter for field ifPartitionNotExists
2024-12-13 11:45:33,186 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - Class class org.apache.flink.sql.parser.ddl.SqlAddPartitions cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
2024-12-13 11:45:33,187 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.sql.parser.ddl.SqlDropPartitions does not contain a setter for field ifExists
2024-12-13 11:45:33,187 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - Class class org.apache.flink.sql.parser.ddl.SqlDropPartitions cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
2024-12-13 11:45:33,187 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.sql.parser.dql.SqlShowPartitions does not contain a getter for field tableIdentifier
2024-12-13 11:45:33,187 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.sql.parser.dql.SqlShowPartitions does not contain a setter for field tableIdentifier
2024-12-13 11:45:33,187 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - Class class org.apache.flink.sql.parser.dql.SqlShowPartitions cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
2024-12-13 11:45:33,188 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.sql.parser.dml.SqlTruncateTable does not contain a getter for field tableNameIdentifier
2024-12-13 11:45:33,188 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.sql.parser.dml.SqlTruncateTable does not contain a setter for field tableNameIdentifier
2024-12-13 11:45:33,188 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - Class class org.apache.flink.sql.parser.dml.SqlTruncateTable cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
2024-12-13 11:45:33,188 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.sql.parser.dql.SqlShowFunctions does not contain a setter for field requireUser
2024-12-13 11:45:33,188 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - Class class org.apache.flink.sql.parser.dql.SqlShowFunctions cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
2024-12-13 11:45:33,188 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.sql.parser.dql.SqlShowProcedures does not contain a getter for field databaseName
2024-12-13 11:45:33,188 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.sql.parser.dql.SqlShowProcedures does not contain a setter for field databaseName
2024-12-13 11:45:33,188 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - Class class org.apache.flink.sql.parser.dql.SqlShowProcedures cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
2024-12-13 11:45:33,189 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.sql.parser.ddl.SqlReplaceTableAs does not contain a setter for field tableName
2024-12-13 11:45:33,189 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - Class class org.apache.flink.sql.parser.ddl.SqlReplaceTableAs cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
2024-12-13 11:45:52,609 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - Print postgres-cdc connector configuration:
2024-12-13 11:45:52,609 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - hostname = localhost
2024-12-13 11:45:52,609 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - password = ******
2024-12-13 11:45:52,609 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - connector = postgres-cdc
2024-12-13 11:45:52,609 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - port = 5434
2024-12-13 11:45:52,609 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - slot.name = flink_orders
2024-12-13 11:45:52,609 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - database-name = postgres
2024-12-13 11:45:52,609 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - schema-name = public
2024-12-13 11:45:52,609 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - table-name = orders
2024-12-13 11:45:52,609 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - username = postgres
2024-12-13 11:45:52,695 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - Print postgres-cdc connector configuration:
2024-12-13 11:45:52,695 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - hostname = localhost
2024-12-13 11:45:52,695 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - password = ******
2024-12-13 11:45:52,695 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - connector = postgres-cdc
2024-12-13 11:45:52,695 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - port = 5434
2024-12-13 11:45:52,695 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - slot.name = flink_shipments
2024-12-13 11:45:52,695 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - database-name = postgres
2024-12-13 11:45:52,695 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - schema-name = public
2024-12-13 11:45:52,695 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - table-name = shipments
2024-12-13 11:45:52,695 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - username = postgres
2024-12-13 11:45:53,603 INFO  org.apache.flink.client.program.rest.RestClusterClient       [] - Submitting job 'insert-into_default_catalog.default_database.orders_shipments_es' (326381b4719d0adc2c12141cc4865e1f).
2024-12-13 11:45:54,133 INFO  org.apache.flink.client.program.rest.RestClusterClient       [] - Successfully submitted job 'insert-into_default_catalog.default_database.orders_shipments_es' (326381b4719d0adc2c12141cc4865e1f) to 'http://localhost:8081'.
2024-12-13 11:47:40,274 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - Print postgres-cdc connector configuration:
2024-12-13 11:47:40,275 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - hostname = localhost
2024-12-13 11:47:40,275 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - password = ******
2024-12-13 11:47:40,275 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - connector = postgres-cdc
2024-12-13 11:47:40,275 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - port = 5434
2024-12-13 11:47:40,275 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - slot.name = flink_orders
2024-12-13 11:47:40,275 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - database-name = postgres
2024-12-13 11:47:40,275 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - schema-name = public
2024-12-13 11:47:40,275 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - table-name = orders
2024-12-13 11:47:40,275 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - username = postgres
2024-12-13 11:47:40,279 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - Print postgres-cdc connector configuration:
2024-12-13 11:47:40,279 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - hostname = localhost
2024-12-13 11:47:40,279 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - password = ******
2024-12-13 11:47:40,279 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - connector = postgres-cdc
2024-12-13 11:47:40,279 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - port = 5434
2024-12-13 11:47:40,279 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - slot.name = flink_shipments
2024-12-13 11:47:40,279 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - database-name = postgres
2024-12-13 11:47:40,279 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - schema-name = public
2024-12-13 11:47:40,279 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - table-name = shipments
2024-12-13 11:47:40,279 INFO  com.ververica.cdc.connectors.postgres.utils.OptionUtils      [] - username = postgres
2024-12-13 11:47:40,358 INFO  org.apache.flink.client.program.rest.RestClusterClient       [] - Submitting job 'insert-into_default_catalog.default_database.orders_shipments_es' (30b2a9b30593f7d0af7b92325c8a07ba).
2024-12-13 11:47:40,690 INFO  org.apache.flink.client.program.rest.RestClusterClient       [] - Successfully submitted job 'insert-into_default_catalog.default_database.orders_shipments_es' (30b2a9b30593f7d0af7b92325c8a07ba) to 'http://localhost:8081'.
2024-12-13 11:50:20,233 ERROR org.apache.flink.table.gateway.service.operation.OperationManager [] - Failed to execute the operation e10240b1-23b7-41dc-b664-9d456f29e6d7.
org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.orders'.

Table options are:

'connector'='postgres-cdc'
'database-name'='postgres'
'hostname'='localhost'
'password'='******'
'port'='5434'
'schema-name'='public'
'slot.name'='flink_orders'
'table-name'='orders'
'username'='postgres'
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:322) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:454) ~[?:?]
	at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:231) ~[?:?]
	at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:181) ~[?:?]
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.Iterator.foreach(Iterator.scala:937) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.Iterator.foreach$(Iterator.scala:937) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.IterableLike.foreach(IterableLike.scala:70) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.TraversableLike.map(TraversableLike.scala:233) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:181) ~[?:?]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1277) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:862) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.callModifyOperations(OperationExecutor.java:513) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:426) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:207) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1575) [?:?]
Caused by: org.apache.flink.table.api.ValidationException: Connector 'postgres-cdc' can only be used as a source. It cannot be used as a sink.
	at org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:818) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.factories.FactoryUtil.discoverTableFactory(FactoryUtil.java:772) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:317) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	... 29 more
2024-12-13 11:50:20,312 ERROR org.apache.flink.table.gateway.service.SqlGatewayServiceImpl [] - Failed to fetchResults.
org.apache.flink.table.gateway.service.utils.SqlExecutionException: Failed to execute the operation e10240b1-23b7-41dc-b664-9d456f29e6d7.
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.processThrowable(OperationManager.java:414) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:267) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1575) [?:?]
Caused by: org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.orders'.

Table options are:

'connector'='postgres-cdc'
'database-name'='postgres'
'hostname'='localhost'
'password'='******'
'port'='5434'
'schema-name'='public'
'slot.name'='flink_orders'
'table-name'='orders'
'username'='postgres'
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:322) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:454) ~[?:?]
	at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:231) ~[?:?]
	at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:181) ~[?:?]
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.Iterator.foreach(Iterator.scala:937) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.Iterator.foreach$(Iterator.scala:937) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.IterableLike.foreach(IterableLike.scala:70) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.TraversableLike.map(TraversableLike.scala:233) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:181) ~[?:?]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1277) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:862) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.callModifyOperations(OperationExecutor.java:513) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:426) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:207) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	... 7 more
Caused by: org.apache.flink.table.api.ValidationException: Connector 'postgres-cdc' can only be used as a source. It cannot be used as a sink.
	at org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:818) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.factories.FactoryUtil.discoverTableFactory(FactoryUtil.java:772) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:317) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:454) ~[?:?]
	at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:231) ~[?:?]
	at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:181) ~[?:?]
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.Iterator.foreach(Iterator.scala:937) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.Iterator.foreach$(Iterator.scala:937) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.IterableLike.foreach(IterableLike.scala:70) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.TraversableLike.map(TraversableLike.scala:233) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:181) ~[?:?]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1277) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:862) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.callModifyOperations(OperationExecutor.java:513) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:426) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:207) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	... 7 more
2024-12-13 11:50:20,316 ERROR org.apache.flink.table.gateway.rest.handler.statement.FetchResultsHandler [] - Unhandled exception.
org.apache.flink.table.gateway.api.utils.SqlGatewayException: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to fetchResults.
	at org.apache.flink.table.gateway.rest.handler.statement.FetchResultsHandler.handleRequest(FetchResultsHandler.java:85) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler.respondToRequest(AbstractSqlGatewayRestHandler.java:84) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler.respondToRequest(AbstractSqlGatewayRestHandler.java:52) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.runtime.rest.handler.AbstractHandler.respondAsLeader(AbstractHandler.java:196) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.lambda$channelRead0$0(LeaderRetrievalHandler.java:83) ~[flink-dist-1.18.0.jar:1.18.0]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.apache.flink.util.OptionalConsumer.ifPresent(OptionalConsumer.java:45) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:80) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:49) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.runtime.rest.handler.router.RouterHandler.routed(RouterHandler.java:115) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:94) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:55) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:208) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:69) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[flink-dist-1.18.0.jar:1.18.0]
	at java.lang.Thread.run(Thread.java:1575) [?:?]
Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to fetchResults.
	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.fetchResults(SqlGatewayServiceImpl.java:229) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.rest.handler.statement.FetchResultsHandler.handleRequest(FetchResultsHandler.java:83) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	... 48 more
Caused by: org.apache.flink.table.gateway.service.utils.SqlExecutionException: Failed to execute the operation e10240b1-23b7-41dc-b664-9d456f29e6d7.
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.processThrowable(OperationManager.java:414) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:267) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	... 1 more
Caused by: org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.orders'.

Table options are:

'connector'='postgres-cdc'
'database-name'='postgres'
'hostname'='localhost'
'password'='******'
'port'='5434'
'schema-name'='public'
'slot.name'='flink_orders'
'table-name'='orders'
'username'='postgres'
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:322) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:454) ~[?:?]
	at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:231) ~[?:?]
	at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:181) ~[?:?]
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.Iterator.foreach(Iterator.scala:937) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.Iterator.foreach$(Iterator.scala:937) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.IterableLike.foreach(IterableLike.scala:70) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.TraversableLike.map(TraversableLike.scala:233) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:181) ~[?:?]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1277) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:862) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.callModifyOperations(OperationExecutor.java:513) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:426) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:207) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	... 1 more
Caused by: org.apache.flink.table.api.ValidationException: Connector 'postgres-cdc' can only be used as a source. It cannot be used as a sink.
	at org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:818) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.factories.FactoryUtil.discoverTableFactory(FactoryUtil.java:772) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:317) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:454) ~[?:?]
	at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:231) ~[?:?]
	at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:181) ~[?:?]
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.Iterator.foreach(Iterator.scala:937) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.Iterator.foreach$(Iterator.scala:937) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.IterableLike.foreach(IterableLike.scala:70) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.TraversableLike.map(TraversableLike.scala:233) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:181) ~[?:?]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1277) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:862) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.callModifyOperations(OperationExecutor.java:513) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:426) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:207) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	... 1 more
2024-12-13 11:50:20,349 WARN  org.apache.flink.table.client.cli.CliClient                  [] - Could not execute SQL statement.
org.apache.flink.table.client.gateway.SqlExecutionException: Failed to get response for the operation e10240b1-23b7-41dc-b664-9d456f29e6d7.
	at org.apache.flink.table.client.gateway.ExecutorImpl.getFetchResultResponse(ExecutorImpl.java:488) ~[flink-sql-client-1.18.0.jar:1.18.0]
	at org.apache.flink.table.client.gateway.ExecutorImpl.fetchUtilResultsReady(ExecutorImpl.java:448) ~[flink-sql-client-1.18.0.jar:1.18.0]
	at org.apache.flink.table.client.gateway.ExecutorImpl.executeStatement(ExecutorImpl.java:309) ~[flink-sql-client-1.18.0.jar:1.18.0]
	at org.apache.flink.table.client.cli.parser.SqlMultiLineParser.parse(SqlMultiLineParser.java:113) ~[flink-sql-client-1.18.0.jar:1.18.0]
	at org.jline.reader.impl.LineReaderImpl.acceptLine(LineReaderImpl.java:2964) ~[flink-sql-client-1.18.0.jar:1.18.0]
	at org.jline.reader.impl.LineReaderImpl$1.apply(LineReaderImpl.java:3778) ~[flink-sql-client-1.18.0.jar:1.18.0]
	at org.jline.reader.impl.LineReaderImpl.readLine(LineReaderImpl.java:679) ~[flink-sql-client-1.18.0.jar:1.18.0]
	at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:194) [flink-sql-client-1.18.0.jar:1.18.0]
	at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:179) [flink-sql-client-1.18.0.jar:1.18.0]
	at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:121) [flink-sql-client-1.18.0.jar:1.18.0]
	at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:114) [flink-sql-client-1.18.0.jar:1.18.0]
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:169) [flink-sql-client-1.18.0.jar:1.18.0]
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:118) [flink-sql-client-1.18.0.jar:1.18.0]
	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:228) [flink-sql-client-1.18.0.jar:1.18.0]
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:179) [flink-sql-client-1.18.0.jar:1.18.0]
Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
org.apache.flink.table.gateway.api.utils.SqlGatewayException: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to fetchResults.
	at org.apache.flink.table.gateway.rest.handler.statement.FetchResultsHandler.handleRequest(FetchResultsHandler.java:85)
	at org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler.respondToRequest(AbstractSqlGatewayRestHandler.java:84)
	at org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler.respondToRequest(AbstractSqlGatewayRestHandler.java:52)
	at org.apache.flink.runtime.rest.handler.AbstractHandler.respondAsLeader(AbstractHandler.java:196)
	at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.lambda$channelRead0$0(LeaderRetrievalHandler.java:83)
	at java.base/java.util.Optional.ifPresent(Optional.java:178)
	at org.apache.flink.util.OptionalConsumer.ifPresent(OptionalConsumer.java:45)
	at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:80)
	at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:49)
	at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at org.apache.flink.runtime.rest.handler.router.RouterHandler.routed(RouterHandler.java:115)
	at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:94)
	at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:55)
	at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at org.apache.flink.shaded.netty4.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:208)
	at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:69)
	at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at java.base/java.lang.Thread.run(Thread.java:1575)
Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to fetchResults.
	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.fetchResults(SqlGatewayServiceImpl.java:229)
	at org.apache.flink.table.gateway.rest.handler.statement.FetchResultsHandler.handleRequest(FetchResultsHandler.java:83)
	... 48 more
Caused by: org.apache.flink.table.gateway.service.utils.SqlExecutionException: Failed to execute the operation e10240b1-23b7-41dc-b664-9d456f29e6d7.
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.processThrowable(OperationManager.java:414)
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:267)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 more
Caused by: org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.orders'.

Table options are:

'connector'='postgres-cdc'
'database-name'='postgres'
'hostname'='localhost'
'password'='******'
'port'='5434'
'schema-name'='public'
'slot.name'='flink_orders'
'table-name'='orders'
'username'='postgres'
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:322)
	at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:454)
	at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:231)
	at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:181)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:181)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1277)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:862)
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.callModifyOperations(OperationExecutor.java:513)
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:426)
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:207)
	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212)
	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119)
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258)
	... 7 more
Caused by: org.apache.flink.table.api.ValidationException: Connector 'postgres-cdc' can only be used as a source. It cannot be used as a sink.
	at org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:818)
	at org.apache.flink.table.factories.FactoryUtil.discoverTableFactory(FactoryUtil.java:772)
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:317)
	... 29 more

End of exception on server side>]
	at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:646) ~[flink-dist-1.18.0.jar:1.18.0]
	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$6(RestClient.java:626) ~[flink-dist-1.18.0.jar:1.18.0]
	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1194) ~[?:?]
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:526) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1575) ~[?:?]
